- [OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/abs/2503.18492)
    - JThomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Flammarion, Maksym Andriushchenko
    - ğŸ›ï¸ Institutions: EPFL, Carnegie Mellon University
    - ğŸ“… Date: Oct. 29, 2025
    - ğŸ“‘ Publisher: NeurIPS 2025 Spotlight (Datasets and Benchmarks Track)
    - ğŸ’» Env: [Desktop]
    - ğŸ”‘ Key: [benchmark], [evaluation], [misuse], [injection]
    - ğŸ“– TLDR: Computer use agents, LLM-based systems interacting with GUIs via screenshots or accessibility trees, lack safety evaluation. This paper introduces OS-HARM, a benchmark built on OSWorld with 150 tasks across three harm categories: deliberate misuse, prompt injection, and model misbehavior (e.g., harassment, data exfiltration). An automated judge assesses both accuracy and safety. Frontier models (like o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro) evaluated show high compliance with misuse queries, vulnerability to static prompt injections, and occasional unsafe actions. [Github](https://github.com/tml-epfl/os-harm)

- [macOSWorld: A Multilingual Interactive Benchmark for GUI Agents](https://arxiv.org/abs/2506.04135)
    - Pei Yang, Hai Ci, and Mike Zheng Shou
    - ğŸ›ï¸ Institutions: NUS
    - ğŸ“… Date: Jun. 4, 2025
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Desktop]
    - ğŸ”‘ Key: [benchmark], [attack]
    - ğŸ“– TLDR: Introduces macOSWorld, the first interactive benchmark for GUI agents on macOS, with 202 tasks across 30 apps (28 macOS-exclusive) in 5 languages plus a safety subset for deception attacks; evaluates 6 agents, showing proprietary CUAs outperform open-source and VLM-based agents, significant language gaps, and both grounding and safety challenges. [Github](https://macos-world.github.io/)

- [Attacking Vision-Language Computer Agents via Pop-ups](https://arxiv.org/abs/2411.02391)
    - Yanzhe Zhang, Tao Yu, Diyi Yang
    - ğŸ›ï¸ Institutions: Georgia Tech, HKU, Stanford
    - ğŸ“… Date: Nov. 4, 2024
    - ğŸ“‘ Publisher: arXiv
    - ğŸ’» Env: [Web], [Desktop]
    - ğŸ”‘ Key: [risk], [pop-ups], [attack]
    - ğŸ“– TLDR: This paper demonstrates that vision-language model (VLM) agents can be easily deceived by carefully designed adversarial pop-ups, leading them to perform unintended actions such as clicking on these pop-ups instead of completing their assigned tasks. Integrating these pop-ups into environments like OSWorld and VisualWebArena resulted in an average attack success rate of 86% and a 47% decrease in task success rate. Basic defense strategies, such as instructing the agent to ignore pop-ups or adding advertisement notices, were found to be ineffective against these attacks. [Github](https://github.com/SALT-NLP/PopupAttack)
